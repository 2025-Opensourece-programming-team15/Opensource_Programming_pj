import requests
from bs4 import BeautifulSoup
import time    # ëœë¤ ë”œë ˆì´ì‹œ
import random  # ëœë¤ ë”œë ˆì´ì‹œ
import re  # ì •ê·œ í‘œí˜„ì‹
import pandas as pd # Pandas df ì‚¬ìš©

# ----------------------
# 1. ìƒìˆ˜ ì •ì˜ (PC ë²„ì „)
# ----------------------
BASE_URL = "https://gall.dcinside.com"

# User-Agent ëª©ë¡ ì •ì˜(ëœë¤ì„ íƒ)
USER_AGENT_LIST = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 14_2_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
]

# robots.txtì— ëª…ì‹œëœ í¬ë¡¤ë§ ê¸ˆì§€(Disallow) ê°¤ëŸ¬ë¦¬ ID ëª©ë¡ ì •ì˜
# ì´ ëª©ë¡ì€ '/board/lists/?id=' ë˜ëŠ” '/mgallery/board/lists/?id='ë¡œ ê¸ˆì§€ëœ IDì…ë‹ˆë‹¤.
DISALLOWED_IDS = {
    '47', 'singo', 'stock_new', 'cat', 'dog', 'baseball_new8', 'm_entertainer1',
    'stock_new2', 'ib_new', 'd_fighter_new1', 'produce48', 'sportsseoul', 
    'metakr', 'salgoonews', 'rezero'
}

def get_regular_post_data(gallery_id: str, gallery_type: str = "minor", search_keyword: str = "", search_option: int = 0, start_page: int = 1, end_page: int = 3) -> pd.DataFrame:
    """
    PC ê°¤ëŸ¬ë¦¬ í˜ì´ì§€ì—ì„œ ê²Œì‹œë¬¼ì˜ ì œëª©ê³¼ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    
    data_list = []

    # robots.txt disallow í•„í„°ë§
    if gallery_id in DISALLOWED_IDS:
        print(f"\nğŸš¨ ê²½ê³ : ê°¤ëŸ¬ë¦¬ ID '{gallery_id}'ëŠ” robots.txtì— ì˜í•´ í¬ë¡¤ë§ì´ ê¸ˆì§€ëœ IDì…ë‹ˆë‹¤. ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return pd.DataFrame(data_list)

    # ê°¤ëŸ¬ë¦¬ ì¢…ë¥˜ë³„ ì£¼ì†Œ ì„¤ì •
    if gallery_type == "minor":
        gallery_type_url = "/mgallery/board/lists"
    elif gallery_type == "major":
        gallery_type_url = "/board/lists"
    elif gallery_type == "mini":
        gallery_type_url = "/mini/board/lists"
    else:
        print("gallery_type ì¸ìê°€ ì˜ëª» ë˜ì—ˆìŠµë‹ˆë‹¤. ë¹ˆ dfë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return pd.DataFrame(data_list)
    
    for i in range(start_page, end_page + 1):
        
        # ----------------------
        # 1ë‹¨ê³„: ëª©ë¡ í˜ì´ì§€ ìš”ì²­ ë° íŒŒì‹±
        # ----------------------
        
        params = {'id': gallery_id, 'page': i}

        # ê²€ìƒ‰ ì£¼ì†Œ ì¡°ë¦½ ì‹œ í•„ìš”í•œ íŒŒë¼ë¯¸í„° ì •ì˜
        # ex) https://gall.dcinside.com/mgallery/board/lists/?id={GalleryID}&s_type={search_option}&s_keyword={search_keyword}
        if search_keyword:
            # PC ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì‚¬ìš©
            params['search_pos'] = ''

            # ê²€ìƒ‰ ì˜µì…˜ ë³„ ì£¼ì†Œ ì„¤ì •
            if search_option == 0:
                params['s_type'] = 'search_subject_memo'
            elif search_option == 1:
                params['s_type'] = 'search_subject'
            elif search_option == 2:
                params['s_type'] = 'search_memo'
            else:
                print("search_option ì¸ìˆ˜ê°€ ì˜ëª» ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì¸ 0(ì œëª©, ë‚´ìš© ê²€ìƒ‰)ìœ¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.")
                params['s_type'] = 'search_subject_memo'
                
            params['s_keyword'] = search_keyword

        # User-Agent ì„¤ì •
        user_agent = random.choice(USER_AGENT_LIST)
        headers = {'User-Agent': user_agent}

        # try-except
        try:
            print(f"--- ê°¤ëŸ¬ë¦¬ ëª©ë¡ í˜ì´ì§€ {i} ìš”ì²­ ì¤‘ ---")
            full_url = BASE_URL + gallery_type_url
            response = requests.get(full_url, params=params, headers=headers, timeout=10)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"ëª©ë¡ í˜ì´ì§€ {i} ìš”ì²­ ì‹¤íŒ¨: {e}. ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
            time.sleep(random.uniform(2, 4))
            continue

        # lxml íŒŒì„œ ì‚¬ìš©(HTML ëŒ€ì‹ )
        soup = BeautifulSoup(response.content, 'lxml')
        
        # ê¸€ ëª©ë¡ êµ¬ì¡°: <tbody> ë‚´ì˜ <tr>
        article_list = soup.find('tbody').find_all('tr', {'data-type': ['icon_pic', 'icon_txt']})
        
        # ê¸°ë³¸ ê³µì§€, ê´‘ê³ ê¸€ í•„í„°ë§
        # ì¼ë°˜ì ìœ¼ë¡œ ì—†ì–´ë„ ë¬´ê´€í•˜ì§€ë§Œ ê³µë°± ê²€ìƒ‰ì‹œ í¬í•¨ë¨
        filtered_articles = []
        for tr_item in article_list:
            writer_tag = tr_item.find('td', class_='gall_writer')
            is_operator_post = writer_tag and writer_tag.get('user_name') == 'ìš´ì˜ì'
            is_notice = tr_item.get('data-type') == 'icon_notice'
            
            if not is_operator_post and not is_notice:
                filtered_articles.append(tr_item)
                
        if not filtered_articles:
             print(f"í˜ì´ì§€ {i}ì—ì„œ ìœ íš¨í•œ ì¼ë°˜ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
             break 


        # ----------------------
        # 2ë‹¨ê³„: ê°œë³„ ê²Œì‹œë¬¼ ì ‘ê·¼ ë° ë‚´ìš© ì¶”ì¶œ 
        # ----------------------
        for tr_item in filtered_articles:
            
            title_tag = tr_item.find('a', href=True)
            if not title_tag: continue

            title_raw = title_tag.text.strip()
            relative_url = title_tag['href']

            # ê²Œì‹œê¸€ ID ì €ì¥
            post_id_match = re.search(r'&no=(\d+)', relative_url)
            post_id = post_id_match.group(1) if post_id_match else None

            # ê²Œì‹œê¸€ ID ì˜¤ë¥˜ ì‹œ ê±´ë„ˆë›°ê¸°
            if not post_id:
                print(f"    -> ì˜¤ë¥˜: ê²Œì‹œë¬¼ ë²ˆí˜¸ ì¶”ì¶œ ì‹¤íŒ¨ ({BASE_URL + relative_url}). ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            
            # href ì ˆëŒ€ ê²½ë¡œ/ìƒëŒ€ ê²½ë¡œ ëª¨ë‘ ëŒ€ì‘ (ì—†ì–´ë„ ì†”ì§íˆ ë¬¸ì œ ì—†ì„ë“¯?)
            if relative_url.startswith('http'):
                full_url = relative_url
            else:
                full_url = BASE_URL + relative_url

            # ëœë¤ ë”œë ˆì´
            time.sleep(random.uniform(3, 5))
            
            # ê²Œì‹œë¬¼ ë³¸ë¬¸ ìš”ì²­
            try:
                print(f"   -> ê²Œì‹œë¬¼ ìš”ì²­: {title_raw[:20]}...")
                article_user_agent = random.choice(USER_AGENT_LIST)
                article_headers = {'User-Agent': article_user_agent}
                article_response = requests.get(full_url, headers=article_headers, timeout=10)
                article_response.raise_for_status()
            except requests.exceptions.RequestException as e:
                print(f"   -> ê²Œì‹œë¬¼ ìš”ì²­ ì‹¤íŒ¨ ({full_url}): {e}")
                continue
            
            article_soup = BeautifulSoup(article_response.content, 'lxml') # lxml ì‚¬ìš©

            # ë³¸ë¬¸ ì¶”ì¶œ í´ë˜ìŠ¤: 'write_div'
            article_contents_tag = article_soup.find('div', class_='write_div')
            article_contents = ""
            if article_contents_tag:
                # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                article_contents = article_contents_tag.get_text(strip=True)
            
            # ----------------------
            # 3ë‹¨ê³„: ë°ì´í„° í´ë¦¬ë‹ ë° ì €ì¥
            # ----------------------
            
            # ì œëª©ê³¼ ê²Œì‹œê¸€ì—ì„œ url ì œê±°
            pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$\-@\.&+:/?=]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
            repl = ''
            title_clean = re.sub(pattern=pattern, repl=repl, string=title_raw).strip()
            article_contents_clean = re.sub(pattern=pattern, repl=repl, string=article_contents).strip()
            
            # '- dc official App' ì œê±°
            article_contents_clean = article_contents_clean.replace('- dc official App', '').strip()
            
            
            if article_contents_clean:
                data_list.append({
                    'PostID': post_id,
                    'Title': title_clean,
                    'Content': article_contents_clean,
                    'GalleryID': gallery_id,
                    'URL': full_url
                })

    # ----------------------
    # 4ë‹¨ê³„: ë¦¬ìŠ¤íŠ¸ë¥¼ ìµœì¢… DataFrameìœ¼ë¡œ ë³€í™˜ ë° ì¤‘ë³µ ì œê±°
    # ----------------------
    df = pd.DataFrame(data_list)

    # PostIDë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ í–‰ ì œê±° (í˜ì´ì§€ê°€ ê²¹ì³ì„œ ì¬ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ ì œê±°)
    if not df.empty:
        df = df.drop_duplicates(subset=['GalleryID', 'PostID'], keep='first')
        print(f"\n--- í¬ë¡¤ë§ ì™„ë£Œ ë° ì¤‘ë³µ ì œê±° ---")
        print(f"ì´ ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ ìˆ˜: {len(data_list)}ê°œ")
        print(f"ì¤‘ë³µ ì œê±° í›„ ìµœì¢… ê²Œì‹œë¬¼ ìˆ˜: {len(df)}ê°œ")
             
    return df